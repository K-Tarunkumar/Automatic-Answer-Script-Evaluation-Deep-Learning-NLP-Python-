{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-12T17:38:12.578425Z",
     "iopub.status.busy": "2023-03-12T17:38:12.577028Z",
     "iopub.status.idle": "2023-03-12T17:38:12.591404Z",
     "shell.execute_reply": "2023-03-12T17:38:12.590075Z",
     "shell.execute_reply.started": "2023-03-12T17:38:12.578369Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, Conv1D, MaxPooling1D, Bidirectional, Concatenate, Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:38:17.056765Z",
     "iopub.status.busy": "2023-03-12T17:38:17.055868Z",
     "iopub.status.idle": "2023-03-12T17:39:45.662943Z",
     "shell.execute_reply": "2023-03-12T17:39:45.661621Z",
     "shell.execute_reply.started": "2023-03-12T17:38:17.056696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('/kaggle/input/answerscript2/p1.csv')\n",
    "\n",
    "# Keep only the essay and domain1_score columns\n",
    "data = data[['essay', 'domain1_score']]\n",
    "\n",
    "# Remove rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Define the maximum length of the essay (in words)\n",
    "MAX_ESSAY_LENGTH = 500\n",
    "\n",
    "# Define the maximum number of sentences per essay\n",
    "MAX_SENTENCES = 50\n",
    "\n",
    "# Define the maximum length of each sentence (in words)\n",
    "MAX_SENTENCE_LENGTH = 20\n",
    "\n",
    "# Preprocess the essays\n",
    "def preprocess_essay(essay):\n",
    "    # Convert to lowercase\n",
    "    essay = essay.lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    essay = re.sub(\"[^a-z\\s+]\",\"\",essay)\n",
    "    # Split the essay into sentences\n",
    "    sentences = essay.split('.')\n",
    "    # Remove leading/trailing white space from each sentence\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    # Remove empty sentences\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    # Truncate the essay to the maximum number of sentences\n",
    "    sentences = sentences[:MAX_SENTENCES]\n",
    "    # Tokenize each sentence\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    # Pad the sequences to the maximum sentence length\n",
    "    sequences = pad_sequences(sequences, maxlen=MAX_SENTENCE_LENGTH)\n",
    "    # Pad the sentences to the maximum number of sentences\n",
    "    num_pad_sentences = MAX_SENTENCES - len(sentences)\n",
    "    padded_sequences = pad_sequences([sequences], maxlen=MAX_SENTENCES, padding='post', truncating='post')\n",
    "    if num_pad_sentences > 0:\n",
    "        padded_sequences = np.append(padded_sequences, np.zeros((num_pad_sentences, MAX_SENTENCE_LENGTH)), axis=0)\n",
    "    return padded_sequences[0]\n",
    "\n",
    "def preprocess_text(x, remove_stopwords=False):\n",
    "    x = x.lower()\n",
    "    x = re.sub(\"[^a-z\\s+]\",\"\",x)\n",
    "    if remove_stopwords:\n",
    "        x = \" \".join([word for word in x.split() if word not in stopwords.words('english')])\n",
    "    return x\n",
    "\n",
    "# Apply the preprocess_essay function to each essay in the dataset\n",
    "data['essay'] = data['essay'].apply(preprocess_text, remove_stopwords=True)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X = np.array(data['essay'].tolist())\n",
    "y = np.array(data['domain1_score'].tolist())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:34:15.926736Z",
     "iopub.status.busy": "2023-03-12T17:34:15.925983Z",
     "iopub.status.idle": "2023-03-12T17:34:15.942009Z",
     "shell.execute_reply": "2023-03-12T17:34:15.940574Z",
     "shell.execute_reply.started": "2023-03-12T17:34:15.926696Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28/964094849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:42:11.287254Z",
     "iopub.status.busy": "2023-03-12T17:42:11.286346Z",
     "iopub.status.idle": "2023-03-12T17:42:11.557983Z",
     "shell.execute_reply": "2023-03-12T17:42:11.556890Z",
     "shell.execute_reply.started": "2023-03-12T17:42:11.287206Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Dropout\n",
    "\n",
    "# Define the input shape for each sentence\n",
    "sentence_input_shape = (MAX_SENTENCE_LENGTH,)\n",
    "\n",
    "# Define the input shape for each essay\n",
    "essay_input_shape = (MAX_SENTENCES, MAX_SENTENCE_LENGTH)\n",
    "\n",
    "# Define the number of LSTM units\n",
    "NUM_LSTM_UNITS = 256\n",
    "\n",
    "# Define the dropout rate\n",
    "DROPOUT_RATE = 0.5\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20)\n",
    "tokenizer.fit_on_texts(data['essay'])\n",
    "\n",
    "# Define the vocabulary size\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define the embedding dimension\n",
    "EMBEDDING_DIM = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:42:16.145957Z",
     "iopub.status.busy": "2023-03-12T17:42:16.145513Z",
     "iopub.status.idle": "2023-03-12T17:42:16.151122Z",
     "shell.execute_reply": "2023-03-12T17:42:16.149977Z",
     "shell.execute_reply.started": "2023-03-12T17:42:16.145922Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Lambda, TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:42:18.825927Z",
     "iopub.status.busy": "2023-03-12T17:42:18.825518Z",
     "iopub.status.idle": "2023-03-12T17:42:18.884859Z",
     "shell.execute_reply": "2023-03-12T17:42:18.883617Z",
     "shell.execute_reply.started": "2023-03-12T17:42:18.825896Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/answerscript2/p1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:42:21.226064Z",
     "iopub.status.busy": "2023-03-12T17:42:21.225668Z",
     "iopub.status.idle": "2023-03-12T17:42:21.239764Z",
     "shell.execute_reply": "2023-03-12T17:42:21.238724Z",
     "shell.execute_reply.started": "2023-03-12T17:42:21.226032Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ind = df.loc[df['essay_set']==1]\n",
    "\n",
    "df_ind = df_ind[['essay','domain1_score']]\n",
    "\n",
    "df_ind = df_ind.sort_values(by= [\"domain1_score\"], ascending=False)\n",
    "answer_sheet = df_ind.iloc[0]['essay']\n",
    "df_ind = df_ind.drop(0)\n",
    "students_answers = list(df['essay'].values)\n",
    "marks_org = list(df['domain1_score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:44:44.870112Z",
     "iopub.status.busy": "2023-03-12T17:44:44.869708Z",
     "iopub.status.idle": "2023-03-12T17:44:45.144223Z",
     "shell.execute_reply": "2023-03-12T17:44:45.143245Z",
     "shell.execute_reply.started": "2023-03-12T17:44:44.870081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of words to consider in the vocabulary\n",
    "NUM_WORDS = 20000\n",
    "\n",
    "# Define the maximum sequence length\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "# Define the sentence input shape\n",
    "sentence_input_shape = (MAX_SEQUENCE_LENGTH,)\n",
    "\n",
    "texts = [essay for essay in data['essay']]\n",
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Define the vocabulary size\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define the embedding dimension\n",
    "EMBEDDING_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:44:53.700968Z",
     "iopub.status.busy": "2023-03-12T17:44:53.699929Z",
     "iopub.status.idle": "2023-03-12T17:45:40.959276Z",
     "shell.execute_reply": "2023-03-12T17:45:40.958002Z",
     "shell.execute_reply.started": "2023-03-12T17:44:53.700912Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data[~data['domain1_score'].isna()]\n",
    "\n",
    "data['essay'] = data['essay'].apply(preprocess_text, remove_stopwords=True)\n",
    "data['expected']=answer_sheet\n",
    "train_df, test_df = train_test_split(data, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_expected_train = pad_sequences(tokenizer.texts_to_sequences(train_df['expected']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_actual_train = pad_sequences(tokenizer.texts_to_sequences(train_df['essay']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_train = train_df['domain1_score'].values / 12\n",
    "\n",
    "X_expected_val = pad_sequences(tokenizer.texts_to_sequences(val_df['expected']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_actual_val = pad_sequences(tokenizer.texts_to_sequences(val_df['essay']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_val = val_df['domain1_score'].values / 12\n",
    "\n",
    "X_expected_test = pad_sequences(tokenizer.texts_to_sequences(test_df['expected']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_actual_test = pad_sequences(tokenizer.texts_to_sequences(test_df['essay']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_test = test_df['domain1_score'].values / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:17:00.455333Z",
     "iopub.status.busy": "2023-03-12T16:17:00.454877Z",
     "iopub.status.idle": "2023-03-12T16:17:01.177610Z",
     "shell.execute_reply": "2023-03-12T16:17:01.175817Z",
     "shell.execute_reply.started": "2023-03-12T16:17:00.455295Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'lambda_2' (type Lambda).\n\nCan not squeeze dim[1], expected a dimension of 1, got 500 for '{{node time_distributed_8/lambda_2/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[1]](time_distributed_8/Reshape)' with input shapes: [?,500].\n\nCall arguments received by layer 'lambda_2' (type Lambda):\n  • inputs=tf.Tensor(shape=(None, 500), dtype=float32)\n  • mask=None\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27/3925751010.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#doc_embedding = TimeDistributed(Lambda(lambda x: sentence_prediction(x)))(document_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#doc_embedding = TimeDistributed(Lambda(lambda x: sentence_prediction(tf.keras.backend.squeeze(x, axis=2))))(document_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mdoc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#doc_embedding = TimeDistributed(sentence_prediction)(document_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27/3925751010.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#doc_embedding = TimeDistributed(Lambda(lambda x: sentence_prediction(x)))(document_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#doc_embedding = TimeDistributed(Lambda(lambda x: sentence_prediction(tf.keras.backend.squeeze(x, axis=2))))(document_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mdoc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#doc_embedding = TimeDistributed(sentence_prediction)(document_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'lambda_2' (type Lambda).\n\nCan not squeeze dim[1], expected a dimension of 1, got 500 for '{{node time_distributed_8/lambda_2/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[1]](time_distributed_8/Reshape)' with input shapes: [?,500].\n\nCall arguments received by layer 'lambda_2' (type Lambda):\n  • inputs=tf.Tensor(shape=(None, 500), dtype=float32)\n  • mask=None\n  • training=None"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Dense, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Lambda, TimeDistributed\n",
    "\n",
    "# Define the list of texts\n",
    "texts = [essay for essay in data['essay']]\n",
    "\n",
    "# Define the number of words to consider in the vocabulary\n",
    "NUM_WORDS = 20000\n",
    "\n",
    "# Define the maximum sequence length\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "# Define the sentence input shape\n",
    "sentence_input_shape = (MAX_SEQUENCE_LENGTH,)\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Define the vocabulary size\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Define the embedding dimension\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Define the sentence-level model\n",
    "sentence_input = Input(shape=sentence_input_shape, dtype='int32', name='sentence_input')\n",
    "sentence_embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='sentence_embedding')(sentence_input)\n",
    "lstm_layer = LSTM(NUM_LSTM_UNITS, return_sequences=True)\n",
    "lstm_output = lstm_layer(sentence_embedding)\n",
    "dropout_layer = Dropout(DROPOUT_RATE)\n",
    "dropout_output = dropout_layer(lstm_output)\n",
    "sentence_prediction = TimeDistributed(Dense(1, activation='sigmoid'))(dropout_output)\n",
    "\n",
    "# Define the document-level model\n",
    "document_input = Input(shape=(None, 500))\n",
    "#document_input = Input(shape=(None, MAX_SEQUENCE_LENGTH), dtype='int32', name='document_input')\n",
    "#doc_embedding = TimeDistributed(Lambda(lambda x: sentence_prediction(x)))(document_input)\n",
    "#doc_embedding = TimeDistributed(Lambda(lambda x: sentence_prediction(tf.keras.backend.squeeze(x, axis=2))))(document_input)\n",
    "doc_embedding = TimeDistributed(Lambda(lambda x: sentence_prediction(tf.squeeze(x, axis=1))))(document_input)\n",
    "\n",
    "#doc_embedding = TimeDistributed(sentence_prediction)(document_input)\n",
    "doc_lstm_layer = LSTM(NUM_LSTM_UNITS)\n",
    "doc_lstm_output = doc_lstm_layer(doc_embedding)\n",
    "doc_dropout_layer = Dropout(DROPOUT_RATE)\n",
    "doc_dropout_output = doc_dropout_layer(doc_lstm_output)\n",
    "document_prediction = Dense(1, activation='sigmoid')(doc_dropout_output)\n",
    "\n",
    "# Define the model inputs and outputs\n",
    "model_inputs = [document_input]\n",
    "model_outputs = [document_prediction]\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=model_inputs, outputs=model_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(x=X_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T16:16:05.785395Z",
     "iopub.status.busy": "2023-03-12T16:16:05.784943Z",
     "iopub.status.idle": "2023-03-12T16:16:05.822672Z",
     "shell.execute_reply": "2023-03-12T16:16:05.820927Z",
     "shell.execute_reply.started": "2023-03-12T16:16:05.785357Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27/1245684951.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_expected_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_actual_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_expected_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_actual_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_expected_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_actual_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x=X_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:46:04.842731Z",
     "iopub.status.busy": "2023-03-12T17:46:04.841952Z",
     "iopub.status.idle": "2023-03-12T17:46:05.789463Z",
     "shell.execute_reply": "2023-03-12T17:46:05.787839Z",
     "shell.execute_reply.started": "2023-03-12T17:46:04.842689Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"time_distributed\" (type TimeDistributed).\n\nLayer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'time_distributed/Reshape:0' shape=(None, 500) dtype=int32>]\n\nCall arguments received by layer \"time_distributed\" (type TimeDistributed):\n  • inputs=tf.Tensor(shape=(None, None, 500), dtype=int32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28/2497910559.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdocument_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'document_input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#document_input = Input(shape=document_input_shape, dtype='int32', name='document_input')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdocument_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mdocument_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_LSTM_UNITS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdocument_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         raise ValueError(\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;34mf\"Inputs received: {inputs}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"time_distributed\" (type TimeDistributed).\n\nLayer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'time_distributed/Reshape:0' shape=(None, 500) dtype=int32>]\n\nCall arguments received by layer \"time_distributed\" (type TimeDistributed):\n  • inputs=tf.Tensor(shape=(None, None, 500), dtype=int32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Define the embedding dimension\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Define the sentence-level model\n",
    "\n",
    "sentence_input = Input(shape=sentence_input_shape, dtype='int32', name='sentence_input')\n",
    "sentence_embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SENTENCE_LENGTH)(sentence_input)\n",
    "sentence_lstm = LSTM(NUM_LSTM_UNITS)(sentence_embedding)\n",
    "sentence_dropout = Dropout(DROPOUT_RATE)(sentence_lstm)\n",
    "\n",
    "exp_sentence_input = Input(shape=sentence_input_shape, dtype='int32', name='exp_sentence_input')\n",
    "exp_sentence_embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SENTENCE_LENGTH)(exp_sentence_input)\n",
    "exp_sentence_lstm = LSTM(NUM_LSTM_UNITS)(exp_sentence_embedding)\n",
    "exp_sentence_dropout = Dropout(DROPOUT_RATE)(exp_sentence_lstm)\n",
    "model_inputs = [exp_sentence_input, sentence_input]\n",
    "sentence_model = Model(inputs=model_inputs, outputs=sentence_dropout)\n",
    "\n",
    "# Define the document-level model\n",
    "#document_input = Input(shape=(None, 500))\n",
    "document_input = Input(shape=(None, MAX_SEQUENCE_LENGTH), dtype='int32', name='document_input')\n",
    "#document_input = Input(shape=document_input_shape, dtype='int32', name='document_input')\n",
    "document_embedding = TimeDistributed(sentence_model)(document_input)\n",
    "document_lstm = LSTM(NUM_LSTM_UNITS)(document_embedding)\n",
    "document_dropout = Dropout(DROPOUT_RATE)(document_lstm)\n",
    "document_output = Dense(1, activation='sigmoid')(document_dropout)\n",
    "\n",
    "exp_document_input = Input(shape=(None, MAX_SEQUENCE_LENGTH), dtype='int32', name='exp_document_input')\n",
    "#document_input = Input(shape=document_input_shape, dtype='int32', name='document_input')\n",
    "exp_document_embedding = TimeDistributed(sentence_model)(exp_document_input)\n",
    "exp_document_lstm = LSTM(NUM_LSTM_UNITS)(exp_document_embedding)\n",
    "exp_document_dropout = Dropout(DROPOUT_RATE)(exp_document_lstm)\n",
    "exp_document_output = Dense(1, activation='sigmoid')(exp_document_dropout)\n",
    "model_inputs2 = [exp_document_input, document_input]\n",
    "document_model = Model(inputs=model_inputs2, outputs=document_output)\n",
    "\n",
    "# Compile the model\n",
    "document_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "# Train the model\n",
    "'''document_model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mse, mae = document_model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('Test Loss:', loss)\n",
    "print('Test MSE:', mse)\n",
    "print('Test MAE:', mae)'''\n",
    "\n",
    "\n",
    "# Compile the model with MAE as the loss function\n",
    "\n",
    "#model = Model(inputs=model_inputs, outputs=model_outputs)\n",
    "\n",
    "# Compile the model with MAE as the loss function\n",
    "#model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae'])\n",
    "document_model.fit(x=[X_expected_train, X_actual_train], y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=([X_expected_val, X_actual_val], y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = document_model.evaluate([X_expected_test, X_actual_test], y_test, batch_size=BATCH_SIZE)\n",
    "print('Test Loss:', loss)\n",
    "print('Test MAE:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T17:34:43.457164Z",
     "iopub.status.busy": "2023-03-12T17:34:43.456710Z",
     "iopub.status.idle": "2023-03-12T17:34:55.617990Z",
     "shell.execute_reply": "2023-03-12T17:34:55.616349Z",
     "shell.execute_reply.started": "2023-03-12T17:34:43.457124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras-Preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from Keras-Preprocessing) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from Keras-Preprocessing) (1.21.6)\n",
      "Installing collected packages: Keras-Preprocessing\n",
      "Successfully installed Keras-Preprocessing-1.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install Keras-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T16:57:55.202396Z",
     "iopub.status.busy": "2023-03-10T16:57:55.201982Z",
     "iopub.status.idle": "2023-03-10T16:57:56.444545Z",
     "shell.execute_reply": "2023-03-10T16:57:56.443454Z",
     "shell.execute_reply.started": "2023-03-10T16:57:55.202363Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
